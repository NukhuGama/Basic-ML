{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "122acfef-7afa-4755-ad06-e833b9de003c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sastrawi in c:\\users\\hp-01\\python_env\\lib\\site-packages (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP-01\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP-01\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mengimpor pustaka google_play_scraper untuk mengakses ulasan dan informasi aplikasi dari Google Play Store.\n",
    "from google_play_scraper import app, reviews, Sort, reviews_all\n",
    " \n",
    "import pandas as pd  # Pandas untuk manipulasi dan analisis data\n",
    "pd.options.mode.chained_assignment = None  # Menonaktifkan peringatan chaining\n",
    "import numpy as np  # NumPy untuk komputasi numerik\n",
    "seed = 0\n",
    "np.random.seed(seed)  # Mengatur seed untuk reproduktibilitas\n",
    "import matplotlib.pyplot as plt  # Matplotlib untuk visualisasi data\n",
    "import seaborn as sns  # Seaborn untuk visualisasi data statistik, mengatur gaya visualisasi\n",
    "from sklearn.metrics import accuracy_score\n",
    " \n",
    "import datetime as dt  # Manipulasi data waktu dan tanggal\n",
    "import re  # Modul untuk bekerja dengan ekspresi reguler\n",
    "import string  # Berisi konstanta string, seperti tanda baca\n",
    "from nltk.tokenize import word_tokenize  # Tokenisasi teks\n",
    "from nltk.corpus import stopwords  # Daftar kata-kata berhenti dalam teks\n",
    " \n",
    "# !pip install sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory  # Stemming (penghilangan imbuhan kata) dalam bahasa Indonesia\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory  # Menghapus kata-kata berhenti dalam bahasa Indonesia\n",
    " \n",
    "from wordcloud import WordCloud  # Membuat visualisasi berbentuk awan kata (word cloud) dari teks\n",
    " \n",
    "import nltk  # Import pustaka NLTK (Natural Language Toolkit).\n",
    "nltk.download('punkt')  # Mengunduh dataset yang diperlukan untuk tokenisasi teks.\n",
    "nltk.download('stopwords')  # Mengunduh dataset yang berisi daftar kata-kata berhenti (stopwords) dalam berbagai bahasa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ddb68-ea1e-4674-b0e3-fe299679fb81",
   "metadata": {},
   "source": [
    "#### Scraping Dataset\n",
    "Scraping data adalah cara untuk mengambil informasi dari halaman web dengan otomatis. Hal ini seperti menyapu (scrape) data dari sebuah situs web, mirip seperti cara Anda mengambil informasi dari buku atau majalah dengan membacanya sekilas.\n",
    "Ketika ingin menganalisis ulasan atau pendapat orang tentang sebuah aplikasi di Google Play Store, kita bisa menggunakan teknik scraping untuk mengumpulkan ulasan-ulasan tersebut secara otomatis. Ini memungkinkan kita untuk memiliki banyak data yang bisa dianalisis lebih lanjut.\n",
    "Ketika mengambil data dari Google Play Store atau situs web lainnya, kita harus berhati-hati untuk tidak melanggar peraturan. Kadang-kadang, situs web memiliki aturan yang melarang pengambilan data secara otomatis. Jadi, pastikan untuk membaca dan mengikuti aturan-aturan tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d2a0d-18dd-4269-970a-1e77d62e22ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengimpor pustaka google_play_scraper untuk mengakses ulasan dan informasi aplikasi dari Google Play Store.\n",
    "from google_play_scraper import app, reviews_all, Sort\n",
    " \n",
    "# Mengambil semua ulasan dari aplikasi dengan ID 'com.byu.id' di Google Play Store.\n",
    "# Proses scraping mungkin memerlukan beberapa saat tergantung pada jumlah ulasan yang ada.\n",
    "scrapreview = reviews_all(\n",
    "    'com.byu.id',          # ID aplikasi\n",
    "    lang='id',             # Bahasa ulasan (default: 'en')\n",
    "    country='id',          # Negara (default: 'us')\n",
    "    sort=Sort.MOST_RELEVANT, # Urutan ulasan (default: Sort.MOST_RELEVANT)\n",
    "    count=1000             # Jumlah maksimum ulasan yang ingin diambil\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7779bd6f-61a9-444f-8b66-4dae1f589e57",
   "metadata": {},
   "source": [
    "Kode di atas kita gunakan untuk mengambil semua ulasan dari sebuah aplikasi di Google Play Store dengan ID 'com.byu.id'. Kita menggunakan pustaka google_play_scraper untuk mengakses ulasan dan informasi aplikasi dari Google Play Store. Dalam kode ini, kita menggunakan fungsi reviews_all() untuk mengambil semua ulasan dari aplikasi tersebut. Proses ini mungkin memerlukan beberapa saat untuk menyelesaikan tugasnya, terutama jika ada banyak ulasan yang perlu diambil. \n",
    "Anda juga dapat menyesuaikan bahasa ulasan (dalam contoh ini, kita menggunakan bahasa Indonesia) dan negara (dalam contoh ini, kita menggunakan Indonesia) sesuai  dengan kebutuhan. Pengguna juga dapat menentukan jumlah maksimum ulasan yang ingin diambil (dalam contoh ini, 1000 ulasan). Ulasan akan diurutkan berdasarkan relevansi, ini berarti ulasan yang dianggap paling relevan akan ditampilkan terlebih dahulu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739da93-a9de-4877-8be5-fccb6b6eae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan ulasan dalam file CSV\n",
    "import csv\n",
    " \n",
    "with open('ulasan_aplikasi.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Review'])  # Menulis header kolom\n",
    "    for review in scrapreview:\n",
    "        writer.writerow([review['content']])  # Menulis konten ulasan ke dalam file CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998a0a1b-be89-4748-a6bb-a5a4c94420dd",
   "metadata": {},
   "source": [
    "#### Loading Dataset\n",
    "Selanjutnya, jika sudah berhasil, kita dapat melihat dataset tersebut dengan kode seperti berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4acb1e-1aaf-4cfe-bbc7-87c44f4adfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_reviews_df = pd.DataFrame(scrapreview)\n",
    "app_reviews_df.shape\n",
    "app_reviews_df.head()\n",
    "app_reviews_df.to_csv('ulasan_aplikasi.csv', index=False)\n",
    " \n",
    "# Membuat DataFrame dari hasil scrapreview\n",
    "app_reviews_df = pd.DataFrame(scrapreview)\n",
    " \n",
    "# Menghitung jumlah baris dan kolom dalam DataFrame\n",
    "jumlah_ulasan, jumlah_kolom = app_reviews_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d001e9-a339-487f-9707-a8e90cddd12c",
   "metadata": {},
   "source": [
    "Kode ini untuk mengambil ulasan aplikasi menggunakan scrapreview dan menyimpannya dalam sebuah DataFrame. Setelah itu, kita menampilkan jumlah baris dan kolom dalam dataset serta lima baris pertama untuk tinjauan cepat. Akhirnya, dataset disimpan dengan format CSV dan nama 'ulasan_aplikasi.csv' untuk digunakan dalam analisis selanjutnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a361c3-ad56-4607-ba43-14a75fa4d5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan lima baris pertama dari DataFrame app_reviews_df\n",
    "app_reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa07b6eb-9147-4770-8ee7-77ebaf08abab",
   "metadata": {},
   "source": [
    "Kode ini menampilkan lima baris pertama dari DataFrame app_reviews_df. DataFrame adalah struktur data tabular yang digunakan dalam pemrograman Python dan metode .head() digunakan untuk melihat beberapa baris awal dari DataFrame tersebut. Hasilnya berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0647650-0559-4d6a-bb01-74cfdcecf368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan informasi tentang DataFrame app_reviews_df\n",
    "app_reviews_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e456c76b-5e7d-412f-824f-65c770313d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat DataFrame baru (clean_df) dengan menghapus baris yang memiliki nilai yang hilang (NaN) dari app_reviews_df\n",
    "clean_df = app_reviews_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f5b631-be3d-456f-8c7e-e39882b6b1e1",
   "metadata": {},
   "source": [
    "Kode ini membuat DataFrame baru disebut clean_df dari DataFrame app_reviews_df dengan menghapus baris dengan nilai yang hilang (NaN). Metode dropna() digunakan untuk menghapus baris dengan setidaknya satu nilai yang hilang. Dengan menggunakan langkah ini, kita dapat membersihkan dataset dari baris yang tidak lengkap sehingga memastikan keberlangsungan analisis data untuk lebih konsisten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf377c-d08f-4818-af4a-d1d326452485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghapus baris duplikat dari DataFrame clean_df\n",
    "clean_df = clean_df.drop_duplicates()\n",
    " \n",
    "# Menghitung jumlah baris dan kolom dalam DataFrame clean_df setelah menghapus duplikat\n",
    "jumlah_ulasan_setelah_hapus_duplikat, jumlah_kolom_setelah_hapus_duplikat = clean_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbf65b8-24f5-4336-8afb-ffb940fc4b6f",
   "metadata": {},
   "source": [
    "Kode ini untuk menghapus baris duplikat dari DataFrame clean_df. Dengan menggunakan metode .drop_duplicates(), baris-baris yang identik akan dihapus, menjaga hanya satu baris dari setiap entri unik. Selanjutnya, kita menghitung jumlah baris dan kolom dalam DataFrame clean_df setelah proses penghapusan duplikat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9705e81d-b887-4f86-a0d8-8dae8f4ba09a",
   "metadata": {},
   "source": [
    "Dengan menghapus nilai NaN dan duplikat, dataset telah dibersihkan dan siap untuk masuk ke tahapan preprocessing teks. Dataset yang bersih akan memberikan dasar solid untuk analisis teks yang akurat dan dapat diandalkan. Sekarang, kita dapat melanjutkan pada langkah-langkah preprocessing teks sesuai dengan kebutuhan analisis kita. Dengan dataset yang sudah dipersiapkan, kita dapat memperoleh wawasan berharga dari ulasan aplikasi tersebut.\n",
    "\n",
    "### Preprocessing Text\n",
    "Dalam analisis teks, langkah penting yang harus dilakukan sebelum memulai pemodelan adalah preprocessing text. Preprocessing text adalah proses membersihkan, memproses, dan mempersiapkan teks mentah agar dapat diolah lebih lanjut dengan tepat dan akurat. Langkah-langkah preprocessing ini bertujuan menghilangkan noise, mengonversi teks ke format yang konsisten, serta mengekstraksi fitur-fitur penting untuk analisis lebih lanjut.\n",
    "\n",
    "Fungsi-fungsi yang disediakan akan sangat berguna untuk langkah-langkah preprocessing teks. Mari kita jelaskan masing-masing fungsi dengan lebih singkat.\n",
    "\n",
    "cleaningText(text): Membersihkan teks dengan menghapus mention, hashtag, RT (retweet), tautan (link), angka, dan tanda baca. Selain itu, karakter newline diganti dengan spasi dan spasi ekstra pada awal dan akhir teks dihapus.\n",
    "casefoldingText(text): Mengonversi semua karakter dalam teks menjadi huruf kecil (lowercase) untuk membuat teks menjadi seragam.\n",
    "tokenizingText(text): Memecah teks menjadi daftar kata atau token. Ini membantu dalam mengurai teks menjadi komponen-komponen dasar untuk analisis lebih lanjut.\n",
    "filteringText(text): Menghapus kata-kata berhenti (stopwords) dalam teks. Daftar kata-kata berhenti telah diperbarui dengan beberapa kata tambahan.\n",
    "stemmingText(text): Menerapkan stemming pada teks, yakni mengurangi kata-kata menjadi bentuk dasarnya. Anda menggunakan pustaka Sastrawi untuk melakukan stemming dalam bahasa Indonesia.\n",
    "toSentence(list_words): Menggabungkan daftar kata-kata menjadi sebuah kalimat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8157fb7d-c324-4e7a-946f-3554b3eefc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    " \n",
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # menghapus mention\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # menghapus hashtag\n",
    "    text = re.sub(r'RT[\\s]', '', text) # menghapus RT\n",
    "    text = re.sub(r\"http\\S+\", '', text) # menghapus link\n",
    "    text = re.sub(r'[0-9]+', '', text) # menghapus angka\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # menghapus karakter selain huruf dan angka\n",
    " \n",
    "    text = text.replace('\\n', ' ') # mengganti baris baru dengan spasi\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # menghapus semua tanda baca\n",
    "    text = text.strip(' ') # menghapus karakter spasi dari kiri dan kanan teks\n",
    "    return text\n",
    " \n",
    "def casefoldingText(text): # Mengubah semua karakter dalam teks menjadi huruf kecil\n",
    "    text = text.lower()\n",
    "    return text\n",
    " \n",
    "def tokenizingText(text): # Memecah atau membagi string, teks menjadi daftar token\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    " \n",
    "def filteringText(text): # Menghapus stopwords dalam teks\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    listStopwords1 = set(stopwords.words('english'))\n",
    "    listStopwords.update(listStopwords1)\n",
    "    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',\"di\",\"ga\",\"ya\",\"gaa\",\"loh\",\"kah\",\"woi\",\"woii\",\"woy\"])\n",
    "    filtered = []\n",
    "    for txt in text:\n",
    "        if txt not in listStopwords:\n",
    "            filtered.append(txt)\n",
    "    text = filtered\n",
    "    return text\n",
    " \n",
    "def stemmingText(text): # Mengurangi kata ke bentuk dasarnya yang menghilangkan imbuhan awalan dan akhiran atau ke akar kata\n",
    "    # Membuat objek stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    " \n",
    "    # Memecah teks menjadi daftar kata\n",
    "    words = text.split()\n",
    " \n",
    "    # Menerapkan stemming pada setiap kata dalam daftar\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    " \n",
    "    # Menggabungkan kata-kata yang telah distem\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    " \n",
    "    return stemmed_text\n",
    " \n",
    "def toSentence(list_words): # Mengubah daftar kata menjadi kalimat\n",
    "    sentence = ' '.join(word for word in list_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d2b8e-e132-4092-81df-54719d691292",
   "metadata": {},
   "source": [
    "Dengan menggunakan fungsi-fungsi ini, Anda dapat membersihkan, memproses, dan mempersiapkan teks sebelum melakukan analisis sentimen atau tugas analisis teks lainnya. Pastikan untuk memanggil fungsi-fungsi ini dengan benar sesuai dengan langkah-langkah preprocessing teks yang Anda butuhkan dalam proyek Anda.\n",
    "\n",
    "Selanjutnya adalah penghapus kumpulan slang words atau kata-kata informal yang sering digunakan dalam percakapan sehari-hari, terutama pada media sosial atau obrolan online. Setiap kata slang memiliki padanan atau substitusi dengan kata formal atau baku. Misalnya, \"abis\" merupakan singkatan dari \"habis\", \"wtb\" merupakan singkatan dari \"beli\", dan seterusnya. Kamus ini berguna untuk membantu pemahaman dan interpretasi teks yang menggunakan bahasa informal atau slang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0869928-459b-4d79-863d-de355d563113",
   "metadata": {},
   "outputs": [],
   "source": [
    "slangwords = {\"@\": \"di\", \"abis\": \"habis\", \"wtb\": \"beli\", \"masi\": \"masih\", \"wts\": \"jual\", \"wtt\": \"tukar\", \"bgt\": \"banget\", \"maks\": \"maksimal\" …}\n",
    "def fix_slangwords(text):\n",
    "    words = text.split()\n",
    "    fixed_words = []\n",
    " \n",
    "    for word in words:\n",
    "        if word.lower() in slangwords:\n",
    "            fixed_words.append(slangwords[word.lower()])\n",
    "        else:\n",
    "            fixed_words.append(word)\n",
    " \n",
    "    fixed_text = ' '.join(fixed_words)\n",
    "    return fixed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b9ab3-9d24-49f1-a251-54ca6f0b22c8",
   "metadata": {},
   "source": [
    "Setelah semua langkah preprocessing telah ditetapkan, langkah berikutnya adalah menerapkannya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51643e6b-3169-4082-b4d6-d0c9b0379638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membersihkan teks dan menyimpannya di kolom 'text_clean'\n",
    "clean_df['text_clean'] = clean_df['content'].apply(cleaningText)\n",
    " \n",
    "# Mengubah huruf dalam teks menjadi huruf kecil dan menyimpannya di 'text_casefoldingText'\n",
    "clean_df['text_casefoldingText'] = clean_df['text_clean'].apply(casefoldingText)\n",
    " \n",
    "# Mengganti kata-kata slang dengan kata-kata standar dan menyimpannya di 'text_slangwords'\n",
    "clean_df['text_slangwords'] = clean_df['text_casefoldingText'].apply(fix_slangwords)\n",
    " \n",
    "# Memecah teks menjadi token (kata-kata) dan menyimpannya di 'text_tokenizingText'\n",
    "clean_df['text_tokenizingText'] = clean_df['text_slangwords'].apply(tokenizingText)\n",
    " \n",
    "# Menghapus kata-kata stop (kata-kata umum) dan menyimpannya di 'text_stopword'\n",
    "clean_df['text_stopword'] = clean_df['text_tokenizingText'].apply(filteringText)\n",
    " \n",
    "# Menggabungkan token-token menjadi kalimat dan menyimpannya di 'text_akhir'\n",
    "clean_df['text_akhir'] = clean_df['text_stopword'].apply(toSentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bab68d-f9db-4e8c-a09d-3c1861086030",
   "metadata": {},
   "source": [
    "#### Pelabelan\n",
    "Sebelum masuk ke tahap pemodelan, langkah yang dilakukan adalah pelabelan. Pelabelan adalah proses pemberian kategori atau label pada setiap entri data berdasarkan informasi yang tersedia. Dalam konteks ini, setiap entri dataset diberikan label sentimen berdasarkan analisis teksnya. Dengan demikian, tahapan pelabelan menjadi dasar untuk proses selanjutnya dalam membangun model klasifikasi sentimen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55de32-2ca1-44f0-a672-e17ec99f3b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from io import StringIO\n",
    " \n",
    "# Membaca data kamus kata-kata positif dari GitHub\n",
    "lexicon_positive = dict()\n",
    " \n",
    "response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')\n",
    "# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub\n",
    " \n",
    "if response.status_code == 200:\n",
    "    # Jika permintaan berhasil\n",
    "    reader = csv.reader(StringIO(response.text), delimiter=',')\n",
    "    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0f5b27-97da-47c4-98b6-de2c49713aff",
   "metadata": {},
   "source": [
    "Kode di atas untuk melakukan analisis sentimen pada teks berbahasa Indonesia menggunakan kamus kata-kata positif dan negatif. Pertama, kita mengirim permintaan HTTP untuk mengambil data kamus kata-kata positif dan negatif dari GitHub. \n",
    "Jika permintaan berhasil, kita membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma. Kemudian, setiap baris dalam file CSV dibaca dan kata-kata beserta skornya dimasukkan ke kamus yang sesuai (lexicon_positive untuk kata-kata positif dan lexicon_negative untuk kata-kata negatif)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d747c73-1ded-4465-8434-c1b9864180b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menentukan polaritas sentimen dari tweet\n",
    " \n",
    "def sentiment_analysis_lexicon_indonesia(text):\n",
    "    #for word in text:\n",
    " \n",
    "    score = 0\n",
    "    # Inisialisasi skor sentimen ke 0\n",
    " \n",
    "    for word in text:\n",
    "        # Mengulangi setiap kata dalam teks\n",
    " \n",
    "        if (word in lexicon_positive):\n",
    "            score = score + lexicon_positive[word]\n",
    "            # Jika kata ada dalam kamus positif, tambahkan skornya ke skor sentimen\n",
    " \n",
    "    for word in text:\n",
    "        # Mengulangi setiap kata dalam teks (sekali lagi)\n",
    " \n",
    "        if (word in lexicon_negative):\n",
    "            score = score + lexicon_negative[word]\n",
    "            # Jika kata ada dalam kamus negatif, kurangkan skornya dari skor sentimen\n",
    " \n",
    "    polarity=''\n",
    "    # Inisialisasi variabel polaritas\n",
    " \n",
    "    if (score >= 0):\n",
    "        polarity = 'positive'\n",
    "        # Jika skor sentimen lebih besar atau sama dengan 0, maka polaritas adalah positif\n",
    "    elif (score < 0):\n",
    "        polarity = 'negative'\n",
    "        # Jika skor sentimen kurang dari 0, maka polaritas adalah negatif\n",
    " \n",
    "    # else:\n",
    "    #     polarity = 'neutral'\n",
    "    # Ini adalah bagian yang bisa digunakan untuk menentukan polaritas netral jika diperlukan\n",
    " \n",
    "    return score, polarity\n",
    "    # Mengembalikan skor sentimen dan polaritas teks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e8f19d-9f41-44a7-8013-f0e6d2fb3ec6",
   "metadata": {},
   "source": [
    "Setelah kamus-kamus tersebut terisi, kita mendefinisikan fungsi sentiment_analysis_lexicon_indonesia yang akan menerima teks sebagai input. Dalam fungsi ini, setiap kata pada teks akan diperiksa, ada dalam kamus positif atau negatif. Jika ada, skor sentimen akan ditambahkan atau dikurangkan sesuai dengan skor kata tersebut dalam kamus. Setelah semua kata diperiksa, skor sentimen akan digunakan untuk menentukan polaritas teks, positif atau negatif. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842efa09-f79e-40e5-af58-0c3cb151946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = clean_df['text_stopword'].apply(sentiment_analysis_lexicon_indonesia)\n",
    "results = list(zip(*results))\n",
    "clean_df['polarity_score'] = results[0]\n",
    "clean_df['polarity'] = results[1]\n",
    "print(clean_df['polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dd8356-c08f-4e12-9b67-d0fca0453c3d",
   "metadata": {},
   "source": [
    "Pada DataFrame clean_df, ada 423 teks dengan polaritas negatif dan 173 teks dengan polaritas positif. Nah, setelah mendapatkan jumlah pasti dari penyebaran label dataset, selanjutnya kita akan melakukan eksplorasi jumlahnya. Dari hasil analisis, 71% dari teks memiliki polaritas negatif, sementara 29% memiliki polaritas positif. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83703507-f8ac-4978-8711-2de1ed5b2b9f",
   "metadata": {},
   "source": [
    "#### Eksplorasi Label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d6a02e-7e48-4d72-8d7d-72120d2fedfe",
   "metadata": {},
   "source": [
    "Lalu, bagaimana penyebaran kata-kata dalam setiap labelnya? Mari kita cek! \n",
    "Teman-teman, bagaimana pendapat Anda terkait visualisasi berikut? Berikut adalah visualisasi dari kata-kata yang sering muncul pada dataset review aplikasi By.U. Visualisasi ini menggunakan WorldCloud. WordCloud adalah representasi visual dari kata-kata yang muncul dalam teks, ketika ukurannya menunjukkan seberapa sering kata tersebut muncul. \n",
    "Nah, bagaimana jika Anda menulis pendapat dan analisis pada Forum Diskusi? Mungkin saja kita bisa melihat persepsi teman-teman yang lainnya. Kue ape kue cucur, meluncuuurr!\n",
    "Di bawah ini, ada tiga visualisasi, yaitu (1) WordCloud secara general; (2) WordCloud untuk Positive Tweets Data; dan (3) Word Cloud untuk Negative Tweets Data. Berikan analisis Anda terhadap tiga gambar tersebut secara lengkap dan komprehensif! Kami tunggu di Forum Diskusi, ya. ????\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6552d9-c60d-40ba-9382-e89c130b468e",
   "metadata": {},
   "source": [
    "#### Data Splitting dan Ekstraksi Fitur dengan TF-IDF\n",
    "Nah, setelah berhasil menetapkan label untuk setiap entri dalam dataset, sekarang kita dapat melangkah ke tahap berikutnya, yaitu ekstraksi fitur dan pemisahan data. Apakah teman-teman masih ingat tentang ekstraksi fitur yang telah kita pelajari sebelumnya? Pada contoh kali ini, kita akan menggunakan metode ekstraksi fitur yang disebut TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35213fa6-8a4a-49dc-8db6-a67accf4d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pisahkan data menjadi fitur (tweet) dan label (sentimen)\n",
    "X = clean_df['text_akhir']\n",
    "y = clean_df['polarity']\n",
    " \n",
    "# Ekstraksi fitur dengan TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=200, min_df=17, max_df=0.8 )\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    " \n",
    "# Konversi hasil ekstraksi fitur menjadi dataframe\n",
    "features_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
    " \n",
    "# Menampilkan hasil ekstraksi fitur\n",
    "features_df\n",
    " \n",
    "# Bagi data menjadi data latih dan data uji\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab656395-52c5-4b17-8bd6-83779b88b46d",
   "metadata": {},
   "source": [
    "#### Modeling\n",
    "Setelah proses ekstraksi fitur dan pemisahan data, tahapan terakhir adalah melakukan pemodelan. Pada contoh ini, kita menggunakan empat algoritma yang berbeda: Naive Bayes (NB), Random Forest (RF), Logistic Regression (LR), dan Decision Tree (DT). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86162f02-e928-434a-986c-a4e4cd3f1bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    " \n",
    "# Membuat objek model Naive Bayes (Bernoulli Naive Bayes)\n",
    "naive_bayes = BernoulliNB()\n",
    " \n",
    "# Melatih model Naive Bayes pada data pelatihan\n",
    "naive_bayes.fit(X_train.toarray(), y_train)\n",
    " \n",
    "# Prediksi sentimen pada data pelatihan dan data uji\n",
    "y_pred_train_nb = naive_bayes.predict(X_train.toarray())\n",
    "y_pred_test_nb = naive_bayes.predict(X_test.toarray())\n",
    " \n",
    "# Evaluasi akurasi model Naive Bayes\n",
    "accuracy_train_nb = accuracy_score(y_pred_train_nb, y_train)\n",
    "accuracy_test_nb = accuracy_score(y_pred_test_nb, y_test)\n",
    " \n",
    "# Menampilkan akurasi\n",
    "print('Naive Bayes - accuracy_train:', accuracy_train_nb)\n",
    "print('Naive Bayes - accuracy_test:', accuracy_test_nb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
